{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "\n",
    "# Packages for cleaning data\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import log\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility for cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText(s, stopword = 0, freqword = 0, rareword = 0, stem_lemma = -1, spellcheck = 0):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\[\\d+\\]',' ',s)\n",
    "    s = re.sub(r\"[^\\w\\s]\",' ',s)\n",
    "    \n",
    "    # Remove stopping words\n",
    "    if stopword == 1:\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        s = \" \".join([word for word in str(s).split() if word not in STOPWORDS])\n",
    "                      \n",
    "    # Remove of frequent words\n",
    "    if freqword == 1:\n",
    "        cnt = Counter()\n",
    "        for word in s.split():\n",
    "            cnt[word] += 1\n",
    "        FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "        s = \" \".join([word for word in str(s).split() if word not in FREQWORDS])\n",
    "    \n",
    "    if rareword == 1:\n",
    "        cnt = Counter()\n",
    "        for word in s.split():\n",
    "            cnt[word] += 1\n",
    "        n_rare_words = 10\n",
    "        RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "        s = \" \".join([word for word in str(s).split() if word not in RAREWORDS])\n",
    "        \n",
    "    # Stemming or LemmatizationÂ¶\n",
    "    if stem_lemma == 1:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        s = \" \".join([lemmatizer.lemmatize(word) for word in s.split()])\n",
    "        \n",
    "    elif stem_lemma == 0:\n",
    "        stemmer = PorterStemmer()\n",
    "        s = \" \".join([stemmer.stem(word) for word in s.split()])\n",
    "    \n",
    "    # Spelling check:\n",
    "    if spellcheck == 1:\n",
    "        spell = SpellChecker()\n",
    "        corrected_text = []\n",
    "        misspelled_words = spell.unknown(s.split())\n",
    "        for word in s.split():\n",
    "            if word in misspelled_words:\n",
    "                corrected_text.append(spell.correction(word))\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        s = \" \".join(corrected_text)\n",
    "#     print(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Onions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onion 192\n"
     ]
    }
   ],
   "source": [
    "onion_txt = []\n",
    "for root, dirs, files in os.walk('./onion'):\n",
    "    for _file in files:\n",
    "        if fnmatch.fnmatch(_file,'*.txt'):\n",
    "            onion_txt.append(_file)\n",
    "\n",
    "# N is for number of total files, N_onion is number of onion files\n",
    "N_onion = len(onion_txt)\n",
    "N = N_onion \n",
    "print('onion',N)\n",
    "\n",
    "# txtnames contains all files' name for cross-validation purpose\n",
    "txtnames = onion_txt.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa 74\n",
      "asia 83\n",
      "britain 100\n",
      "europe 112\n",
      "international 38\n",
      "latin_america 66\n",
      "north_america 60\n",
      "\n",
      "Numbers of total files:  725 True\n"
     ]
    }
   ],
   "source": [
    "econ_dir = os.listdir('./economist')\n",
    "location_txt = {} # contains all economist txt based on area\n",
    "N_econ = {} # recode the number of files from each area\n",
    "\n",
    "for d in econ_dir:\n",
    "    txt = []\n",
    "    for root, dirs, files in os.walk(f'./economist/{d}/'):\n",
    "        for _file in files:\n",
    "            if fnmatch.fnmatch(_file,'*.txt'):\n",
    "                txt.append(_file)\n",
    "    location_txt[d] = txt\n",
    "    txtnames += txt.copy()\n",
    "\n",
    "for key in location_txt.keys():\n",
    "    print(key, len(location_txt[key]))\n",
    "    N += len(location_txt[key])\n",
    "    N_econ[key] = len(location_txt[key])\n",
    "    \n",
    "print('\\nNumbers of total files: ',N, N==len(txtnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onions 192\n"
     ]
    }
   ],
   "source": [
    "onions = []\n",
    "for txt in onion_txt:\n",
    "    path = \"./onion/\" + txt\n",
    "    s = ''\n",
    "    with open(path, 'r', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            # look at line in loop\n",
    "            line = line.replace(\"\\n\",\" \").strip() + ' '\n",
    "            s += line\n",
    "    clean_s = preprocessingText(s.strip(),stopword = 1, freqword = 1, rareword = 0, stem_lemma = 0, spellcheck=0)\n",
    "    onions.append(list(clean_s.split()))\n",
    "    \n",
    "print('onions',len(onions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa 74\n",
      "asia 83\n",
      "britain 100\n",
      "europe 112\n",
      "international 38\n",
      "latin_america 66\n",
      "north_america 60\n"
     ]
    }
   ],
   "source": [
    "economists = {}\n",
    "for key in location_txt.keys():\n",
    "    temp = []\n",
    "    for txt in location_txt[key]:\n",
    "        path =  f\"./economist/{key}/\" + txt\n",
    "        s = ''\n",
    "        with open(path, 'r', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # look at line in loop\n",
    "                line = line.replace(\"\\n\",\" \").strip() + ' '\n",
    "                s += line\n",
    "        clean_s = preprocessingText(s.strip(),stopword = 1, freqword = 1, rareword = 0, stem_lemma = 0, spellcheck=0)\n",
    "        temp.append(list(clean_s.split()))\n",
    "    economists[key] = temp\n",
    "    print(key,len(economists[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have all clean data for onion and economist \n",
    "  - where onion is a list, and economist is in distribution containing data from different area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def Test_Train_Split_Area(fold, alldata, num, org_n):\n",
    "    \"\"\"\n",
    "    [0,533) = Econ -> 0\n",
    "    [533,735] = Onion -> 1\n",
    "    \"\"\"\n",
    "    accum = list(itertools.accumulate(org_n))\n",
    "    \n",
    "    count_class = [0] * len(org_n)\n",
    "    test = []\n",
    "    for ind in fold:\n",
    "        test.append(alldata[ind])\n",
    "        num.remove(ind)\n",
    "        res = list(map(lambda i: i > ind, accum)).index(True)\n",
    "        count_class[res] += 1\n",
    "        \n",
    "    train = []\n",
    "    for i in num:\n",
    "        train.append(alldata[i])\n",
    "    \n",
    "    counts = [org_n[i] - count_class[i] for i in range(len(org_n))]\n",
    "        \n",
    "    return test, train, counts, num.copy()\n",
    "\n",
    "def counting(train, order):\n",
    "    V = set()\n",
    "    for lst in train:\n",
    "        V.update(lst)\n",
    "    \n",
    "    dic_econ = {}\n",
    "    dic_onion = {}\n",
    "    \n",
    "    C_econ = C_onion = len(V)\n",
    "    \n",
    "    for i,pos in enumerate(order):\n",
    "        cnt = Counter(train[i])\n",
    "        if 0 <= pos < 192:\n",
    "            C_onion += len(train[i])\n",
    "            for k,v in cnt.items():\n",
    "                if k not in dic_onion:\n",
    "                    dic_onion[k] = 0\n",
    "                dic_onion[k] += v\n",
    "        else:\n",
    "            C_econ += len(train[i])\n",
    "            for k,v in cnt.items():\n",
    "                if k not in dic_econ:\n",
    "                    dic_econ[k] = 0\n",
    "                dic_econ[k] += v\n",
    "    return dic_econ,dic_onion,C_econ,C_onion\n",
    "\n",
    "def P_class_doc(P_x_,P_,doc,denom):\n",
    "    log_p = log(P_)\n",
    "    diff = set(doc).difference(P_x_.keys())\n",
    "    for i in doc:\n",
    "        if i in diff:\n",
    "            log_p += log(1/(denom+1))\n",
    "        else:\n",
    "            log_p += log(P_x_[i])\n",
    "    return log_p\n",
    "\n",
    "def CM_Descprition(y_test, y_pred):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    n = len(cm)\n",
    "    \n",
    "    # Precision:\n",
    "    precisions = []\n",
    "    for i in range(n):\n",
    "        c_ii = cm[i][i]\n",
    "        sum_p = 0\n",
    "        for j in range(n):\n",
    "            sum_p += cm[j][i]\n",
    "        if sum_p != 0: \n",
    "            precisions.append(c_ii/sum_p)\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "        \n",
    "    # Recall:\n",
    "    recalls = []\n",
    "    for i in range(n):\n",
    "        c_ii = cm[i][i]\n",
    "        sum_r = 0\n",
    "        for j in range(n):\n",
    "            sum_r += cm[i][j]\n",
    "        if sum_r != 0:\n",
    "            recalls.append(c_ii/sum_r)\n",
    "        else:\n",
    "            recalls.append(0)\n",
    "    \n",
    "    # Accuracy:\n",
    "    accuracies = []\n",
    "    for i in range(n):\n",
    "        accuracies.append(cm[i][i]/sum(map(sum, cm)))    \n",
    "    \n",
    "    # F - test\n",
    "    F = []\n",
    "    for i in range(n):\n",
    "        if precisions[i] + recalls[i] != 0:\n",
    "            F.append(2 * precisions[i] * recalls[i] / (precisions[i] + recalls[i]))\n",
    "        else:\n",
    "            F.append(0)\n",
    "        \n",
    "    return precisions, recalls, accuracies, F\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of fold 0\n",
      "[[52  0]\n",
      " [ 0 20]]\n",
      "Confusion Matrix of fold 1\n",
      "[[49  1]\n",
      " [ 0 22]]\n",
      "Confusion Matrix of fold 2\n",
      "[[50  6]\n",
      " [ 0 16]]\n",
      "Confusion Matrix of fold 3\n",
      "[[51  0]\n",
      " [ 0 21]]\n",
      "Confusion Matrix of fold 4\n",
      "[[51  0]\n",
      " [ 1 20]]\n",
      "Confusion Matrix of fold 5\n",
      "[[51  1]\n",
      " [ 0 20]]\n",
      "Confusion Matrix of fold 6\n",
      "[[55  0]\n",
      " [ 1 16]]\n",
      "Confusion Matrix of fold 7\n",
      "[[51  0]\n",
      " [ 0 21]]\n",
      "Confusion Matrix of fold 8\n",
      "[[55  1]\n",
      " [ 1 15]]\n",
      "Confusion Matrix of fold 9\n",
      "[[59  0]\n",
      " [ 0 18]]\n",
      "Economists\n",
      "Overall Precision:  0.9945054945054945\n",
      "Overall Recall:  0.983576923076923\n",
      "Overall Accuracy:  0.7224567099567099\n",
      "Overall F:  0.9887011588719776\n",
      "====================\n",
      "Onion\n",
      "Overall Precision:  0.9573675418784114\n",
      "Overall Recall:  0.9831057422969188\n",
      "Overall Accuracy:  0.2608766233766234\n",
      "Overall F:  0.9678299522827765\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# combine all economist data\n",
    "econ = []\n",
    "for i in economists.values():\n",
    "    econ += i\n",
    "    \n",
    "alldata = onions + econ # remember this order does matter\n",
    "num = [i for i in range(725)] # number from 0-192 are onino, others are econ\n",
    "random.shuffle(num)\n",
    "\n",
    "folds = []\n",
    "start = 0\n",
    "end = N // 10\n",
    "for i in range(9):\n",
    "    folds.append(num[start:end])\n",
    "    start = end\n",
    "    end += N//10\n",
    "folds.append(num[start:])\n",
    "\n",
    "PRECISIONS = {i:[] for i in range(2)}\n",
    "RECALLS = {i:[] for i in range(2)}\n",
    "ACCURACIES = {i:[] for i in range(2)}\n",
    "FSCORES = {i:[] for i in range(2)}\n",
    "\n",
    "for ind,fold in enumerate(folds):\n",
    "    test,train,counts,order = Test_Train_Split(fold, alldata, num.copy(),[533,192])\n",
    "    n_econ = counts[0]\n",
    "    n_onion = counts[1]\n",
    "\n",
    "    P_econ = n_econ / sum(counts)\n",
    "    P_onion = 1 - P_econ\n",
    "    \n",
    "    dfEcon, dfOnion,denomEcon,denomOnion = counting(train,order)\n",
    "\n",
    "    # Conditional Probabilities\n",
    "    P_x_econ = {i:(dfEcon[i]+1)/denomEcon for i in dfEcon}\n",
    "    P_x_onion = {i:(dfOnion[i]+1)/denomOnion for i in dfOnion}\n",
    "    \n",
    "    # give a test set\n",
    "    y_pred = []\n",
    "    for i in range(len(test)):\n",
    "        P_econ_doc = P_class_doc(P_x_econ,P_econ,test[i],denomEcon)\n",
    "        P_onion_doc = P_class_doc(P_x_onion,P_onion,test[i],denomOnion)\n",
    "        if P_econ_doc < P_onion_doc:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    print('Confusion Matrix of fold {}'.format(ind))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    y_test = [1 if i < 192 else 0 for i in fold]\n",
    "    Precisions, Recalls, Accuracies, F = CM_Descprition(y_test, y_pred)\n",
    "    for i in range(2):\n",
    "        PRECISIONS[i].append(Precisions[i])\n",
    "        RECALLS[i].append(Recalls[i])\n",
    "        ACCURACIES[i].append(Accuracies[i])\n",
    "        FSCORES[i].append(F[i])\n",
    "\n",
    "for ind,v in enumerate(['Economists','Onion']):\n",
    "    print(v)\n",
    "    print('Overall Precision: ', sum(PRECISIONS[ind])/10)\n",
    "    print('Overall Recall: ',sum(RECALLS[ind])/10)\n",
    "    print('Overall Accuracy: ',sum(ACCURACIES[ind])/10)\n",
    "    print('Overall F: ',sum(FSCORES[ind])/10)\n",
    "    \n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2\n",
    "\n",
    "### Summary:\n",
    "<l> The statistic data for both classes, Onion and Economists are very impressive because all data are close to 1. Total accruacy $\\approx$ 0.7233+0.2602  = 0.9835. This indicates that the model perform very well to classify almost all files correctly. As confusion matrix shown, there are only a litter number of false negative and false positive classifications. Overall, this model is great enough to predict two classes.The result is what I expected because I used all words in a test set to do the prediction, which means even though it cost time to calculation the posterior probabilities, also I did remove the stopwords, such as, 'we','the','I', and etc,so that the training and test set has more valuable words for training. But it still has some systematic errors otherwise the model would predict perfectly. These systematic errors perhaps coming from text cleaning, for example, one files has words that two classes gives similar weights, in result, a slightly higher weight causing the result lean to the incorrect direction. This error could be blame to the insufficient text cleaning or over text cleaning, For example, spelling correction is not used in this experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3: Most Representative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = []\n",
    "test,train,counts, order = Test_Train_Split(fold, alldata, num.copy(),[533,192])\n",
    "\n",
    "n_econ,n_onion= counts[0],counts[1]\n",
    "P_econ = n_econ / (n_econ+n_onion)\n",
    "P_onion = 1 - P_econ\n",
    "\n",
    "dfEcon, dfOnion,denomEcon,denomOnion = counting(train,order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year  : 1110  \n",
      "say  : 880  \n",
      "one  : 873  \n",
      "countri  : 669  \n",
      "govern  : 666  \n",
      "==================================================\n",
      "one  : 253  \n",
      "year  : 252  \n",
      "ad  : 243  \n",
      "time  : 243  \n",
      "imag  : 241  \n"
     ]
    }
   ],
   "source": [
    "# Econ\n",
    "k = Counter(dfEcon)\n",
    "high = k.most_common(5)\n",
    "for i in high: \n",
    "    print(i[0],\" :\",i[1],\" \") \n",
    "    \n",
    "print('='*50)\n",
    "# Onion\n",
    "k = Counter(dfOnion)\n",
    "high = k.most_common(5)\n",
    "for i in high: \n",
    "    print(i[0],\" :\",i[1],\" \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "After cleaning the top 10 high frequency words in each class, this results are what I expected they should be are explanable. Economists files metion and use more words from human, government, trade, economist, finance, and etc. As we see, 'countri','govern','say' are used most frequently in the economists class. But'year' and 'one' have a high frequencies showing up in both classes. Therefore, 'year' and 'one' can be a bad words in predictions and causes the false negative and false positive. What I want to list out are words with prefix \"financ-\" and \"econom-\". They are more representitive in Economists class. While Onion class gives the most representative words \"ad (advertisement)\", \"time\", and \"imag\", which makes sense since people need to plant and sell the onion, so advertisement and image are necessary. But again we can change words \"one\" and \"year\" to \"cooking\" and \"spicy\" because onion is a seasoning in our food. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3: Least Representative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnaud  : 1  \n",
      "buf  : 1  \n",
      "doyen  : 1  \n",
      "catchili  : 1  \n",
      "boardroom  : 1  \n",
      "==================================================\n",
      "sanderson  : 1  \n",
      "buford  : 1  \n",
      "storefront  : 1  \n",
      "dime  : 1  \n",
      "lumina  : 1  \n"
     ]
    }
   ],
   "source": [
    "# Econ\n",
    "k = Counter(dfEcon)\n",
    "n = 5\n",
    "low = k.most_common()[:-n-1:-1]\n",
    "for i in low: \n",
    "    print(i[0],\" :\",i[1],\" \") \n",
    "    \n",
    "print('='*50)\n",
    "# Onion\n",
    "k = Counter(dfOnion)\n",
    "low = k.most_common()[:-n-1:-1]\n",
    "for i in low: \n",
    "    print(i[0],\" :\",i[1],\" \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "From both classes, it hard to say why they are the least common words. From my guess, their order in the list are random so they are picked up by chances because as we see, they all have same weights, which suggest that they have same numbers of appearances. However, the word, \"discriminatori\" and \"reboot\" are one of the least common words in the Economist class, it may be because all files are articles online or at least not using in meeting or classes. In Onion class, since words have typos, I think they are normal in this case. Other words since, were cleaned up and lost their original suffix and I cannot tell the exact meaning of them, but overall make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    [0,74) = africa => 0\n",
    "    [74,157) - 74 = [0,83) = asia => 1\n",
    "    [157,257) - 157 = [0,100) = britain => 2\n",
    "    [257,369) - 257 = [0,112) = europe => 3\n",
    "    [369,407) - 369 = [0,38) = international => 4\n",
    "    [407,473) - 407 = [0,66) = latin_america => 5\n",
    "    [473,533) - 473 = [0,60) = north_america => 6\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_area(train, order, org_n):\n",
    "    V = set()\n",
    "    for lst in train:\n",
    "        V.update(lst)\n",
    "    \n",
    "    lst = [{} for _ in range(len(org_n))]\n",
    "    C_class = [len(V)]*len(org_n)\n",
    "    \n",
    "    accum = list(itertools.accumulate(org_n))\n",
    "    \n",
    "    for i,pos in enumerate(order):\n",
    "        cnt = Counter(train[i])\n",
    "        \n",
    "        res = list(map(lambda i: i > pos, accum)).index(True)\n",
    "        C_class[res] += len(train[i])\n",
    "        for k,v in cnt.items():\n",
    "            if k not in lst[res]:\n",
    "                lst[res][k] = 0\n",
    "            lst[res][k] += v\n",
    "              \n",
    "    return lst,C_class.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get 10 fold crossover validations and train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fold or test-train set:\n",
    "econ = []\n",
    "for i in economists.values():\n",
    "    econ += i\n",
    "    \n",
    "N = len(econ)\n",
    "num = [i for i in range(N)]\n",
    "random.shuffle(num)\n",
    "\n",
    "folds = []\n",
    "start = 0\n",
    "end = N // 10\n",
    "for i in range(9):\n",
    "    folds.append(num[start:end])\n",
    "    start = end\n",
    "    end += N//10\n",
    "    \n",
    "folds.append(num[start:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0]\n",
      " [ 0  5  1  3  0  0  0]\n",
      " [ 0  0  8  0  0  0  0]\n",
      " [ 1  0  0  8  0  0  0]\n",
      " [ 1  0  1  1  1  0  0]\n",
      " [ 0  1  0  0  1  9  0]\n",
      " [ 0  0  0  0  0  0  2]]\n",
      "====================\n",
      "[[5 0 0 0 0 0 0]\n",
      " [1 2 0 2 0 0 0]\n",
      " [0 0 8 1 0 0 0]\n",
      " [2 0 1 9 0 1 0]\n",
      " [2 0 1 0 2 0 0]\n",
      " [0 2 1 0 0 5 0]\n",
      " [0 0 2 1 0 1 4]]\n",
      "====================\n",
      "[[4 0 0 0 1 0 0]\n",
      " [1 7 0 1 0 0 0]\n",
      " [0 0 9 1 0 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 4 1 1 0 0]\n",
      " [0 0 0 2 0 3 0]\n",
      " [1 0 2 0 0 1 5]]\n",
      "====================\n",
      "[[ 4  0  0  0  0  0  0]\n",
      " [ 1  7  3  1  1  0  0]\n",
      " [ 0  0 11  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0]\n",
      " [ 1  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  1  6  0]\n",
      " [ 0  0  2  0  0  0  7]]\n",
      "====================\n",
      "[[ 7  0  2  0  1  0  0]\n",
      " [ 0  7  0  0  0  0  0]\n",
      " [ 1  0  9  2  1  0  0]\n",
      " [ 0  0  0 15  0  0  0]\n",
      " [ 1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  1  0  2]]\n",
      "====================\n",
      "[[8 0 0 1 1 0 0]\n",
      " [2 5 0 1 0 0 0]\n",
      " [0 0 9 0 0 0 0]\n",
      " [0 0 0 9 0 0 0]\n",
      " [0 0 1 0 2 0 1]\n",
      " [0 0 1 0 0 3 0]\n",
      " [0 0 1 0 1 0 7]]\n",
      "====================\n",
      "[[5 0 1 0 0 0 0]\n",
      " [2 9 0 2 0 0 0]\n",
      " [0 0 9 0 2 0 0]\n",
      " [1 0 1 8 0 0 1]\n",
      " [2 0 2 0 1 0 0]\n",
      " [0 0 0 0 0 3 0]\n",
      " [0 0 0 0 0 0 4]]\n",
      "====================\n",
      "[[ 7  1  0  1  0  0  0]\n",
      " [ 0  4  0  0  1  0  0]\n",
      " [ 0  0 12  0  0  0  0]\n",
      " [ 0  0  1  9  1  0  0]\n",
      " [ 0  0  3  0  2  0  0]\n",
      " [ 0  0  0  2  0  5  0]\n",
      " [ 0  0  1  0  0  0  3]]\n",
      "====================\n",
      "[[ 8  0  0  1  0  0  0]\n",
      " [ 0  2  1  0  0  0  0]\n",
      " [ 0  0  9  0  0  0  0]\n",
      " [ 0  0  1 13  1  0  0]\n",
      " [ 1  0  0  1  1  0  0]\n",
      " [ 0  0  1  0  0  6  0]\n",
      " [ 0  0  2  0  0  0  5]]\n",
      "====================\n",
      "[[ 5  0  1  0  0  0  0]\n",
      " [ 2  7  0  2  0  0  0]\n",
      " [ 1  0  7  0  0  0  0]\n",
      " [ 0  0  3 10  0  0  0]\n",
      " [ 0  0  1  0  1  0  0]\n",
      " [ 1  0  3  0  0  7  0]\n",
      " [ 0  0  2  0  0  0  3]]\n",
      "====================\n",
      "africa\n",
      "Overall Precision:  0.7188888888888888\n",
      "Overall Recall:  0.8633333333333333\n",
      "Overall Accuracy:  0.11836253369272236\n",
      "Overall F:  0.7695427963849016\n",
      "====================\n",
      "asia\n",
      "Overall Precision:  0.9133333333333333\n",
      "Overall Recall:  0.6692132867132867\n",
      "Overall Accuracy:  0.10306603773584903\n",
      "Overall F:  0.7651301476301476\n",
      "====================\n",
      "britain\n",
      "Overall Precision:  0.6655696509372981\n",
      "Overall Recall:  0.91743783993784\n",
      "Overall Accuracy:  0.17099056603773582\n",
      "Overall F:  0.7648314190668014\n",
      "====================\n",
      "europe\n",
      "Overall Precision:  0.7730144038967567\n",
      "Overall Recall:  0.8762548562548563\n",
      "Overall Accuracy:  0.18200808625336926\n",
      "Overall F:  0.8160501274088231\n",
      "====================\n",
      "international\n",
      "Overall Precision:  0.4833333333333334\n",
      "Overall Recall:  0.275\n",
      "Overall Accuracy:  0.02065363881401617\n",
      "Overall F:  0.34158730158730155\n",
      "====================\n",
      "latin_america\n",
      "Overall Precision:  0.9464285714285715\n",
      "Overall Recall:  0.7858116883116883\n",
      "Overall Accuracy:  0.0936320754716981\n",
      "Overall F:  0.8547741147741148\n",
      "====================\n",
      "north_america\n",
      "Overall Precision:  0.9675\n",
      "Overall Recall:  0.7342063492063492\n",
      "Overall Accuracy:  0.07894204851752021\n",
      "Overall F:  0.8208846872082166\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PRECISIONS = {i:[] for i in range(7)}\n",
    "RECALLS = {i:[] for i in range(7)}\n",
    "ACCURACIES = {i:[] for i in range(7)}\n",
    "FSCORES = {i:[] for i in range(7)}\n",
    "\n",
    "for fold in folds:\n",
    "    test,train,counts,order = Test_Train_Split_Area(fold, econ, num.copy(),list(N_econ.values()))\n",
    "        \n",
    "    P_area = [i/sum(counts) for i in counts]\n",
    "    \n",
    "    dic_area, C_area = counting_area(train,order, list(N_econ.values()))\n",
    "    \n",
    "    # Conditional Probabilities\n",
    "    P_x_area = []\n",
    "    for i in range(7):\n",
    "        _x_area = {j:(dic_area[i][j]+1)/C_area[i] for j in dic_area[i]}\n",
    "        P_x_area.append(_x_area)\n",
    "\n",
    "    # give a test set and predict\n",
    "    y_pred = []\n",
    "    for t in range(len(test)):\n",
    "        p_class_doc = []\n",
    "        for j in range(7):\n",
    "            p = P_class_doc(P_x_area[j],P_area[j],test[t],C_area[j])\n",
    "            p_class_doc.append(p)\n",
    "        \n",
    "        y_pred.append(p_class_doc.index(max(p_class_doc)))\n",
    "\n",
    "    y_test = []\n",
    "    for ind in fold:\n",
    "        if ind < 74:\n",
    "            y_test.append(0)\n",
    "        elif ind < 157:\n",
    "            y_test.append(1)\n",
    "        elif ind < 257:\n",
    "            y_test.append(2)\n",
    "        elif ind < 369:\n",
    "            y_test.append(3)\n",
    "        elif ind < 407:\n",
    "            y_test.append(4)\n",
    "        elif ind < 473:\n",
    "            y_test.append(5)\n",
    "        else:\n",
    "            y_test.append(6)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    Precisions, Recalls, Accuracies, F = CM_Descprition(y_test, y_pred)\n",
    "    for i in range(7):\n",
    "        PRECISIONS[i].append(Precisions[i])\n",
    "        RECALLS[i].append(Recalls[i])\n",
    "        ACCURACIES[i].append(Accuracies[i])\n",
    "        FSCORES[i].append(F[i])\n",
    "    \n",
    "    print('='*20)\n",
    "\n",
    "for ind,v in enumerate(econ_dir):\n",
    "    print(v)\n",
    "    print('Overall Precision: ', sum(PRECISIONS[ind])/10)\n",
    "    print('Overall Recall: ',sum(RECALLS[ind])/10)\n",
    "    print('Overall Accuracy: ',sum(ACCURACIES[ind])/10)\n",
    "    print('Overall F: ',sum(FSCORES[ind])/10)\n",
    "    \n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "From the above data summary we know that, international and britain got lower precision, to some extent, it makes sense because international is a broad words most likely covering or being coverd by other 6 areas. To understand this, we check its recall is also very low, which indicates that the sense is highly likely to be true. While Britain has high recall, which means it has a good prediction in True Positive and a little bad in flase negatives, but it has a high false positives in high probability as the same reason as International. Other areas have high precision in general and over 70% recall score which seems good. We know that there must be systematic errors from classsifications in NLP due to text, the input, I gave to the model. As before, it is the same reansom most likely to the previous experiments between Onion and Economists. But here we need to do more text cleaning work because we are analyzing the classes from the same general topic, the Economist, which results in high similarities of words in 6 areas, if all area all have high weights on common words, then the model would suffer from which class should be choose, and results may become worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3: Most Representative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = []\n",
    "\n",
    "test,train,counts,order = Test_Train_Split_Area(fold, econ, num.copy(),list(N_econ.values()))\n",
    "        \n",
    "P_area = [i/sum(counts) for i in counts]\n",
    "\n",
    "dic_area, C_area = counting_area(train,order,list(N_econ.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa\n",
      "year  : 163  \n",
      "say  : 142  \n",
      "one  : 117  \n",
      "countri  : 108  \n",
      "govern  : 107  \n",
      "====================\n",
      "asia\n",
      "year  : 197  \n",
      "one  : 143  \n",
      "say  : 142  \n",
      "two  : 112  \n",
      "parti  : 110  \n",
      "====================\n",
      "britain\n",
      "one  : 210  \n",
      "year  : 199  \n",
      "use  : 159  \n",
      "say  : 154  \n",
      "make  : 151  \n",
      "====================\n",
      "europe\n",
      "year  : 202  \n",
      "say  : 158  \n",
      "one  : 148  \n",
      "minist  : 138  \n",
      "govern  : 137  \n",
      "====================\n",
      "international\n",
      "countri  : 122  \n",
      "one  : 113  \n",
      "year  : 105  \n",
      "say  : 92  \n",
      "use  : 83  \n",
      "====================\n",
      "latin_america\n",
      "year  : 133  \n",
      "say  : 115  \n",
      "state  : 107  \n",
      "countri  : 92  \n",
      "govern  : 91  \n",
      "====================\n",
      "north_america\n",
      "american  : 111  \n",
      "year  : 111  \n",
      "democrat  : 109  \n",
      "like  : 102  \n",
      "peopl  : 88  \n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for ind,area in enumerate(econ_dir):\n",
    "    print(area)\n",
    "    k = Counter(dic_area[ind])\n",
    "    high = k.most_common(5)    \n",
    "    for i in high: \n",
    "        print(i[0],\" :\",i[1],\" \") \n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "the some words from the top 5 common words are very representative to the corresponding area. For exmaple, in North America, \"american\", \"democrat\", and \"peol\" are strong words to say, \"hay, this is the US.\" But I have to say the some words have high frequencies appearing in different areas, such as \"mr\", \"year\"ï¼\"one\", and \"govern,\" so that they are not representative any more. For Africa, the words related with economy could be \"poor\", \"lagged\", \"underdeveloped\", \"low\" since most of the countries in Africa are mid-developing countries or underdevelpoed countries, even though these years their economy grows up fast, but still have a long path. For Asia, we can give words, such as, \"china\",\"pacific\", \"APEC\" (Asia-Pacific Economic Cooperation), \"manufacturing\", and \"environment\" because there are more business opportunities and jobs in new emerging market especially in China and India, and also \"environment\" is the topic that the developing countries are concerned most. For Britain, such as, \"London\", \"LSEG\", \"Pounds\" can be used. For Latin America, we could say \"Brazil\",\"Cuba\",\"Coco\",\"Cigar\". For North America, we could update the words to \"Trump\", \"Tesla\",\"Twitter\",\"Bloomberg\" and etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3: Least Representative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa\n",
      "bolster  : 1  \n",
      "rancour  : 1  \n",
      "sniper  : 1  \n",
      "prop  : 1  \n",
      "stagnant  : 1  \n",
      "====================\n",
      "asia\n",
      "thrill  : 1  \n",
      "doorway  : 1  \n",
      "golden  : 1  \n",
      "processor  : 1  \n",
      "astrid  : 1  \n",
      "====================\n",
      "britain\n",
      "steel  : 1  \n",
      "stainless  : 1  \n",
      "circuit  : 1  \n",
      "microelectron  : 1  \n",
      "travers  : 1  \n",
      "====================\n",
      "europe\n",
      "underperform  : 1  \n",
      "overstaf  : 1  \n",
      "inflict  : 1  \n",
      "103  : 1  \n",
      "eur700m  : 1  \n",
      "====================\n",
      "international\n",
      "swim  : 1  \n",
      "eclect  : 1  \n",
      "piecem  : 1  \n",
      "prescript  : 1  \n",
      "dogmat  : 1  \n",
      "====================\n",
      "latin_america\n",
      "remit  : 1  \n",
      "endow  : 1  \n",
      "unpaid  : 1  \n",
      "technocrat  : 1  \n",
      "ténico  : 1  \n",
      "====================\n",
      "north_america\n",
      "duck  : 1  \n",
      "contradictori  : 1  \n",
      "perspect  : 1  \n",
      "awkwardli  : 1  \n",
      "rightli  : 1  \n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for ind,area in enumerate(econ_dir):\n",
    "    print(area)\n",
    "    for i in Counter(dic_area[ind]).most_common()[:-n-1:-1]:  \n",
    "        print(i[0],\" :\",i[1],\" \") \n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "It was not surpried to see that these least representative words has lower weight in negative exponential to the 5 at least. Some of words shows up in different areas. As I mentioned in the model, Onion-Economists, these words are least likely to be read in the file, but they were chosen to here by chances. As we here, the result still have many typos therefore they are not representative and make sense to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
