{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBCiAMAAVYbL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\abc33\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCERBsXPVYbY"
   },
   "source": [
    "## Apply ploting function for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2HlWlGsVYbZ"
   },
   "outputs": [],
   "source": [
    "def plot_training(history, metrics=[]):\n",
    "    \"\"\"\n",
    "    Plot training and validation statistics\n",
    "    - accuracy vs epoch number\n",
    "    - loss     vs epoch number\n",
    "\n",
    "    From https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "    \"\"\"  \n",
    "\n",
    "    # Loss\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    for metric in metrics:\n",
    "      metric_value = history.history[metric]\n",
    "      plt.plot(epochs, metric_value, 'b', label=\"Training \" + metric)\n",
    "      plt.title('Training  accuracy')\n",
    "      plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8qkFBDIVYbl"
   },
   "source": [
    "# The Data\n",
    "\n",
    "The dataset is called Fashion MNIST.\n",
    "\n",
    "Rather than classifying images into one of ten digits,\n",
    "you will classify images of clothing items into one of ten classes.\n",
    "\n",
    "Here's the code to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-62x63qHVYbo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Determine \n",
    "# - the dimensions of the input by examining the first training example\n",
    "# - the dimensions of the output (number of classes) by examinimg the targets\n",
    "input_size = np.prod(X_train[0].shape)\n",
    "output_size = np.unique(y_train).shape[0]\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = X_train[0].shape[0:2]\n",
    "\n",
    "valid_size = X_train.shape[0] // 10\n",
    "\n",
    "# Flatten the data to one dimension and normalize to range [0,1]\n",
    "X_train = X_train.astype(np.float32).reshape(-1, input_size) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, input_size) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:valid_size], X_train[valid_size:]\n",
    "y_valid, y_train = y_train[:valid_size], y_train[valid_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vy72Ne0sVYb1"
   },
   "source": [
    "### Create call backs\n",
    "- Early Stopping\n",
    "- Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "po0Rkf42VYb4"
   },
   "outputs": [],
   "source": [
    "modelName = \"class_model\"\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=.00005, patience=2, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "callbacks = [ es_callback,\n",
    "              ModelCheckpoint(filepath=modelName + \".ckpt\", monitor='acc', save_best_only=True)\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpoEEFFPVYcG"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Create a one layer Neural Network to perform classification (e.g., logistic regression) as we demonstrated in class.\n",
    "\n",
    "No layers other than the one for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ltcZhLzRVYcI",
    "outputId": "7336566a-9c8d-4ad9-b5b4-5e13df033424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.6296 - accuracy: 0.7915 - val_loss: 0.5239 - val_accuracy: 0.8363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f520a097cf8>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification\n",
    "class_model = Sequential([ layers.Dense(output_size, activation=\"sigmoid\", input_dim=input_size) ] )\n",
    "class_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "class_model.fit(X_train, y_train, nb_epoch=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_T2xeRHVYcY"
   },
   "source": [
    "## Task 2 (9 parts)\n",
    "\n",
    "You will evaluate out of sample accuracy for several models and create a plot\n",
    "- For number of layers in 2, 3,4 (more if you like)\n",
    "- For number of units per layer in 10, 50, 200 (more if you like)\n",
    "    - Using *same* number of units for each layer\n",
    "\n",
    "The combination of choices for number of layers and number of units per layer gives you (at least) **9 models**.\n",
    "\n",
    "- Evaluate the out of sample accuracy for each of the 9 models.\n",
    "- Present the results in a table (created by code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1TR9QzsmVYca",
    "outputId": "0120d5e4-61f1-4c37-a841-e45852c97ede",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1672/1688 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.5829INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.8665 - acc: 0.5852 - val_loss: 0.5039 - val_acc: 0.8302\n",
      "Epoch 2/10\n",
      "1683/1688 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.8347INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4844 - acc: 0.8347 - val_loss: 0.4551 - val_acc: 0.8472\n",
      "Epoch 3/10\n",
      "1684/1688 [============================>.] - ETA: 0s - loss: 0.4517 - acc: 0.8445INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4516 - acc: 0.8445 - val_loss: 0.4375 - val_acc: 0.8512\n",
      "Epoch 4/10\n",
      "1681/1688 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8499INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4334 - acc: 0.8499 - val_loss: 0.4250 - val_acc: 0.8498\n",
      "Epoch 5/10\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.8534INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4228 - acc: 0.8535 - val_loss: 0.4287 - val_acc: 0.8493\n",
      "Epoch 6/10\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.8555INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4143 - acc: 0.8556 - val_loss: 0.4176 - val_acc: 0.8553\n",
      "Epoch 7/10\n",
      "1673/1688 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8568INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4081 - acc: 0.8568 - val_loss: 0.4161 - val_acc: 0.8540\n",
      "Epoch 8/10\n",
      "1686/1688 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8603INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4006 - acc: 0.8603 - val_loss: 0.4056 - val_acc: 0.8602\n",
      "Epoch 9/10\n",
      "1674/1688 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8609INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3950 - acc: 0.8610 - val_loss: 0.4105 - val_acc: 0.8562\n",
      "Epoch 10/10\n",
      "1668/1688 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8638INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3918 - acc: 0.8638 - val_loss: 0.4184 - val_acc: 0.8488\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_238 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4380 - acc: 0.8480\n",
      "Test dataset: loss=0.4380, accuracy=0.8480\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.8030 - acc: 0.6945 - val_loss: 0.5262 - val_acc: 0.8272\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5037 - acc: 0.8268 - val_loss: 0.4667 - val_acc: 0.8380\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4625 - acc: 0.8390 - val_loss: 0.4461 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4382 - acc: 0.8476 - val_loss: 0.4319 - val_acc: 0.8537\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4256 - acc: 0.8514 - val_loss: 0.4530 - val_acc: 0.8435\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4176 - acc: 0.8545 - val_loss: 0.4145 - val_acc: 0.8582\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4068 - acc: 0.8591 - val_loss: 0.4329 - val_acc: 0.8582\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4004 - acc: 0.8579 - val_loss: 0.4166 - val_acc: 0.8563\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_240 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,070\n",
      "Trainable params: 8,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4431 - acc: 0.8468\n",
      "Test dataset: loss=0.4431, accuracy=0.8468\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.9245 - acc: 0.5604 - val_loss: 0.6967 - val_acc: 0.6435\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.6679 - acc: 0.6499 - val_loss: 0.6258 - val_acc: 0.6620\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.6086 - acc: 0.6635 - val_loss: 0.6017 - val_acc: 0.6622\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5371 - acc: 0.7554 - val_loss: 0.4660 - val_acc: 0.8358\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4509 - acc: 0.8433 - val_loss: 0.4342 - val_acc: 0.8532\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4308 - acc: 0.8501 - val_loss: 0.4741 - val_acc: 0.8358\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4201 - acc: 0.8541 - val_loss: 0.4457 - val_acc: 0.8470\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_243 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,180\n",
      "Trainable params: 8,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4700 - acc: 0.8405\n",
      "Test dataset: loss=0.4700, accuracy=0.8405\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5621 - acc: 0.8073 - val_loss: 0.4410 - val_acc: 0.8500\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4147 - acc: 0.8551 - val_loss: 0.3861 - val_acc: 0.8647\n",
      "Epoch 3/10\n",
      "1668/1688 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8653INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3769 - acc: 0.8655 - val_loss: 0.3983 - val_acc: 0.8602\n",
      "Epoch 4/10\n",
      "1666/1688 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8746INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3527 - acc: 0.8747 - val_loss: 0.3539 - val_acc: 0.8755\n",
      "Epoch 5/10\n",
      "1682/1688 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8797INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3351 - acc: 0.8796 - val_loss: 0.3628 - val_acc: 0.8677\n",
      "Epoch 6/10\n",
      "1674/1688 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8851INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3210 - acc: 0.8849 - val_loss: 0.3387 - val_acc: 0.8740\n",
      "Epoch 7/10\n",
      "1671/1688 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.8884INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3076 - acc: 0.8885 - val_loss: 0.3455 - val_acc: 0.8762\n",
      "Epoch 8/10\n",
      "1684/1688 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8909INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2997 - acc: 0.8909 - val_loss: 0.3613 - val_acc: 0.8682\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_247 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3725 - acc: 0.8629\n",
      "Test dataset: loss=0.3725, accuracy=0.8629\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5630 - acc: 0.8061 - val_loss: 0.4336 - val_acc: 0.8512\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4089 - acc: 0.8555 - val_loss: 0.4253 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3653 - acc: 0.8681 - val_loss: 0.3956 - val_acc: 0.8572\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3412 - acc: 0.8768 - val_loss: 0.3477 - val_acc: 0.8755\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3250 - acc: 0.8810 - val_loss: 0.3410 - val_acc: 0.8770\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3119 - acc: 0.8869 - val_loss: 0.3202 - val_acc: 0.8882\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3007 - acc: 0.8901 - val_loss: 0.3361 - val_acc: 0.8785\n",
      "Epoch 8/10\n",
      "1684/1688 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.8921INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2923 - acc: 0.8920 - val_loss: 0.3306 - val_acc: 0.8803\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_249 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3536 - acc: 0.8738\n",
      "Test dataset: loss=0.3536, accuracy=0.8738\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5617 - acc: 0.8005 - val_loss: 0.4088 - val_acc: 0.8542\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3957 - acc: 0.8559 - val_loss: 0.3958 - val_acc: 0.8550\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3595 - acc: 0.8674 - val_loss: 0.3774 - val_acc: 0.8592\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3342 - acc: 0.8767 - val_loss: 0.3574 - val_acc: 0.8672\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3193 - acc: 0.8824 - val_loss: 0.3332 - val_acc: 0.8773\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3042 - acc: 0.8861 - val_loss: 0.3221 - val_acc: 0.8827\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2925 - acc: 0.8899 - val_loss: 0.3452 - val_acc: 0.8778\n",
      "Epoch 8/10\n",
      "1683/1688 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.8950INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2815 - acc: 0.8950 - val_loss: 0.3345 - val_acc: 0.8835\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_252 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 44,860\n",
      "Trainable params: 44,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3548 - acc: 0.8733\n",
      "Test dataset: loss=0.3548, accuracy=0.8733\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5205 - acc: 0.8188 - val_loss: 0.3932 - val_acc: 0.8637\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3914 - acc: 0.8601 - val_loss: 0.3718 - val_acc: 0.8640\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3463 - acc: 0.8743 - val_loss: 0.3860 - val_acc: 0.8647\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3204 - acc: 0.8824 - val_loss: 0.3325 - val_acc: 0.8768\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2989 - acc: 0.8894 - val_loss: 0.3231 - val_acc: 0.8842\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2850 - acc: 0.8937 - val_loss: 0.3130 - val_acc: 0.8843\n",
      "Epoch 7/10\n",
      "1686/1688 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.8988INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2708 - acc: 0.8988 - val_loss: 0.3106 - val_acc: 0.8870\n",
      "Epoch 8/10\n",
      "1683/1688 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9027INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2582 - acc: 0.9029 - val_loss: 0.3187 - val_acc: 0.8817\n",
      "Epoch 9/10\n",
      "1674/1688 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9078INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2464 - acc: 0.9079 - val_loss: 0.3320 - val_acc: 0.8838\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_256 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3429 - acc: 0.8771\n",
      "Test dataset: loss=0.3429, accuracy=0.8771\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5052 - acc: 0.8206 - val_loss: 0.3999 - val_acc: 0.8600\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3709 - acc: 0.8639 - val_loss: 0.3741 - val_acc: 0.8647\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3329 - acc: 0.8762 - val_loss: 0.3546 - val_acc: 0.8708\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3098 - acc: 0.8858 - val_loss: 0.3317 - val_acc: 0.8817\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2908 - acc: 0.8911 - val_loss: 0.3284 - val_acc: 0.8877\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2759 - acc: 0.8961 - val_loss: 0.3197 - val_acc: 0.8857\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2607 - acc: 0.9018 - val_loss: 0.3142 - val_acc: 0.8862\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2494 - acc: 0.9048 - val_loss: 0.3277 - val_acc: 0.8860\n",
      "Epoch 9/10\n",
      "1684/1688 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9087INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2392 - acc: 0.9087 - val_loss: 0.3085 - val_acc: 0.8933\n",
      "Epoch 10/10\n",
      "1673/1688 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9119INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2287 - acc: 0.9120 - val_loss: 0.3010 - val_acc: 0.9003\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_258 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3375 - acc: 0.8858\n",
      "Test dataset: loss=0.3375, accuracy=0.8858\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5060 - acc: 0.8188 - val_loss: 0.3881 - val_acc: 0.8575\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3733 - acc: 0.8638 - val_loss: 0.3327 - val_acc: 0.8803\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3356 - acc: 0.8771 - val_loss: 0.3389 - val_acc: 0.8768\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3127 - acc: 0.8841 - val_loss: 0.3646 - val_acc: 0.8622\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_261 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 239,410\n",
      "Trainable params: 239,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3675 - acc: 0.8686\n",
      "Test dataset: loss=0.3675, accuracy=0.8686\n"
     ]
    }
   ],
   "source": [
    "Unit = [10,50,200]\n",
    "Layer = [2,3,4]\n",
    "\n",
    "df = pd.DataFrame(columns=Layer, index=Unit)\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in Unit:\n",
    "    for j in Layer:\n",
    "        # create a sequential model\n",
    "            model = Sequential()\n",
    "        for k in range(j-1):\n",
    "                model.add(layers.Dense(i, activation=tf.nn.relu, input_dim=input_size))#这里的代码要改毕竟input_dim不能老是input_size\n",
    "        model.add(layers.Dense(output_size, activation=\"sigmoid\"))\n",
    "        \n",
    "        # compile and fit model\n",
    "        metrics = [ \"acc\" ]\n",
    "        model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=metrics)\n",
    "        model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid),callbacks=callbacks)\n",
    "        \n",
    "        # print summary()\n",
    "        print(model.summary())\n",
    "        \n",
    "        # evaluate model using test set\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "        print(\"Test dataset: loss={tl:5.4f}, accuracy={ta:5.4f}\".format(tl=test_loss, ta=test_accuracy))\n",
    "        \n",
    "        df.loc[i,j]=test_accuracy\n",
    "        \n",
    "        if accuracy < test_accuracy:\n",
    "            accuracy = test_accuracy\n",
    "            myModel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RBtxWNKVYcq"
   },
   "source": [
    "**Following table is the accuracy table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "2W7aRVFEVYcs",
    "outputId": "260a1371-16f5-4fe9-c371-498a9192196f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.848</td>\n",
       "      <td>0.8468</td>\n",
       "      <td>0.8405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.8629</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>0.8733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.8771</td>\n",
       "      <td>0.8858</td>\n",
       "      <td>0.8686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2       3       4\n",
       "10    0.848  0.8468  0.8405\n",
       "50   0.8629  0.8738  0.8733\n",
       "200  0.8771  0.8858  0.8686"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtdPDZobVYc5"
   },
   "source": [
    "**Highest Accuracy and Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "MVuexxkSVYc8",
    "outputId": "6556ae41-bbe4-4888-8ecc-e4c99a8099c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8858000040054321\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_258 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(myModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9kgQDocVYdH"
   },
   "source": [
    "- Create a *single plot*\n",
    "    - Horizontal axis: number of units per layer\n",
    "    - Vertical axis: out of sample accuracy\n",
    "    - 3 traces in the plot, one for each choice of number of layers\n",
    "        - e.g., one line in the plot for L=2, another line in the plot for L=3, and another line for L=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "xvTaJmHXVYdM",
    "outputId": "70257502-ffc3-495d-e108-4e716b06ff9f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEcCAYAAAD0haEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(Unit,df[2], label='2 layers')\n",
    "plt.plot(Unit,df[3], label='3 layers')\n",
    "plt.plot(Unit,df[4], label='4 layers')\n",
    "plt.xticks(Unit)\n",
    "plt.legend(fontsize=10)\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVtd5Y3VXZ2o"
   },
   "source": [
    "### As the graph and summary shown, we know that the highest accuracy is about 0.88% with three layers and 200 neurons for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVQzBr8DVYdZ"
   },
   "source": [
    "# Extra credit\n",
    "- Do a more extensive experiment\n",
    "    - More layers\n",
    "    - More choices for number of units per layer\n",
    "    - Vary number of units per layer\n",
    "        - Increase number of units with increasing layer number\n",
    "        - Decrease number of units with increasing layer number\n",
    "        - Find some other pattern for varying the number of units per layer\n",
    "- Comment on when your models are either overfitting or underfitting\n",
    "    - Use numeric justification\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oT6U8BkVYdc"
   },
   "source": [
    "### Keep the number of units same with increasing layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rsf4XQKeVYdf"
   },
   "outputs": [],
   "source": [
    "Unit2 = [10,50,100,200,300]\n",
    "Layer2 = [2,3,4,5,6]\n",
    "df2 = pd.DataFrame(columns=Layer2, index=Unit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sOuPhQo_VYdt",
    "outputId": "a8d8c135-0942-4acc-d347-2a0df6d60187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.7137 - acc: 0.7483 - val_loss: 0.4983 - val_acc: 0.8355\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4812 - acc: 0.8353 - val_loss: 0.4540 - val_acc: 0.8498\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4534 - acc: 0.8446 - val_loss: 0.4431 - val_acc: 0.8457\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4373 - acc: 0.8502 - val_loss: 0.4346 - val_acc: 0.8520\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4290 - acc: 0.8524 - val_loss: 0.4532 - val_acc: 0.8458\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4198 - acc: 0.8552 - val_loss: 0.4238 - val_acc: 0.8542\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4157 - acc: 0.8559 - val_loss: 0.4632 - val_acc: 0.8413\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4087 - acc: 0.8580 - val_loss: 0.4265 - val_acc: 0.8517\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_265 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_266 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4534 - acc: 0.8414\n",
      "Test dataset: loss=0.4534, accuracy=0.8414\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.7965 - acc: 0.7031 - val_loss: 0.5273 - val_acc: 0.8277\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5003 - acc: 0.8278 - val_loss: 0.4804 - val_acc: 0.8387\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4630 - acc: 0.8399 - val_loss: 0.4528 - val_acc: 0.8445\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4403 - acc: 0.8451 - val_loss: 0.4386 - val_acc: 0.8458\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4291 - acc: 0.8495 - val_loss: 0.4222 - val_acc: 0.8522\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4151 - acc: 0.8539 - val_loss: 0.4355 - val_acc: 0.8528\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4034 - acc: 0.8582 - val_loss: 0.4188 - val_acc: 0.8562\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3971 - acc: 0.8598 - val_loss: 0.4113 - val_acc: 0.8550\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3900 - acc: 0.8611 - val_loss: 0.4207 - val_acc: 0.8507\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3841 - acc: 0.8638 - val_loss: 0.4067 - val_acc: 0.8563\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_267 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_268 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_269 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,070\n",
      "Trainable params: 8,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4302 - acc: 0.8475\n",
      "Test dataset: loss=0.4302, accuracy=0.8475\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.8469 - acc: 0.6792 - val_loss: 0.5621 - val_acc: 0.8027\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5061 - acc: 0.8243 - val_loss: 0.4979 - val_acc: 0.8253\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4610 - acc: 0.8374 - val_loss: 0.4593 - val_acc: 0.8465\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4378 - acc: 0.8445 - val_loss: 0.4371 - val_acc: 0.8505\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4224 - acc: 0.8519 - val_loss: 0.4250 - val_acc: 0.8543\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4112 - acc: 0.8552 - val_loss: 0.4380 - val_acc: 0.8507\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4014 - acc: 0.8589 - val_loss: 0.4160 - val_acc: 0.8540\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3942 - acc: 0.8604 - val_loss: 0.4172 - val_acc: 0.8542\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3890 - acc: 0.8605 - val_loss: 0.4361 - val_acc: 0.8517\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_270 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_272 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,180\n",
      "Trainable params: 8,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4494 - acc: 0.8436\n",
      "Test dataset: loss=0.4494, accuracy=0.8436\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.9508 - acc: 0.5808 - val_loss: 0.6752 - val_acc: 0.7070\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5720 - acc: 0.7961 - val_loss: 0.4976 - val_acc: 0.8293\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5012 - acc: 0.8235 - val_loss: 0.4874 - val_acc: 0.8320\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4737 - acc: 0.8345 - val_loss: 0.4636 - val_acc: 0.8417\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4542 - acc: 0.8404 - val_loss: 0.4552 - val_acc: 0.8402\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4392 - acc: 0.8450 - val_loss: 0.4813 - val_acc: 0.8297\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4251 - acc: 0.8495 - val_loss: 0.4375 - val_acc: 0.8468\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4167 - acc: 0.8522 - val_loss: 0.4122 - val_acc: 0.8562\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4058 - acc: 0.8553 - val_loss: 0.4222 - val_acc: 0.8515\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4026 - acc: 0.8563 - val_loss: 0.4225 - val_acc: 0.8507\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_274 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_276 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_278 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,290\n",
      "Trainable params: 8,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4488 - acc: 0.8457\n",
      "Test dataset: loss=0.4488, accuracy=0.8457\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.8856 - acc: 0.6576 - val_loss: 0.5962 - val_acc: 0.8008\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5559 - acc: 0.8088 - val_loss: 0.5142 - val_acc: 0.8225\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4996 - acc: 0.8258 - val_loss: 0.4785 - val_acc: 0.8320\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4688 - acc: 0.8348 - val_loss: 0.4649 - val_acc: 0.8392\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4466 - acc: 0.8429 - val_loss: 0.4691 - val_acc: 0.8367\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4267 - acc: 0.8478 - val_loss: 0.4239 - val_acc: 0.8538\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4117 - acc: 0.8544 - val_loss: 0.4368 - val_acc: 0.8505\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4016 - acc: 0.8569 - val_loss: 0.4252 - val_acc: 0.8522\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_279 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_280 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_281 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,400\n",
      "Trainable params: 8,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4543 - acc: 0.8394\n",
      "Test dataset: loss=0.4543, accuracy=0.8394\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5665 - acc: 0.8052 - val_loss: 0.4278 - val_acc: 0.8537\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4267 - acc: 0.8498 - val_loss: 0.4210 - val_acc: 0.8492\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3916 - acc: 0.8612 - val_loss: 0.3792 - val_acc: 0.8665\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3635 - acc: 0.8706 - val_loss: 0.3569 - val_acc: 0.8762\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3485 - acc: 0.8757 - val_loss: 0.3459 - val_acc: 0.8755\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3296 - acc: 0.8799 - val_loss: 0.3951 - val_acc: 0.8583\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3173 - acc: 0.8850 - val_loss: 0.3517 - val_acc: 0.8740\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_285 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3793 - acc: 0.8678\n",
      "Test dataset: loss=0.3793, accuracy=0.8678\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5566 - acc: 0.8046 - val_loss: 0.4074 - val_acc: 0.8580\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3994 - acc: 0.8571 - val_loss: 0.3664 - val_acc: 0.8697\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3592 - acc: 0.8706 - val_loss: 0.3588 - val_acc: 0.8688\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3367 - acc: 0.8772 - val_loss: 0.3599 - val_acc: 0.8713\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3193 - acc: 0.8831 - val_loss: 0.3400 - val_acc: 0.8742\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3089 - acc: 0.8859 - val_loss: 0.3474 - val_acc: 0.8712\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2944 - acc: 0.8905 - val_loss: 0.3472 - val_acc: 0.8747\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_287 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_288 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_289 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3661 - acc: 0.8671\n",
      "Test dataset: loss=0.3661, accuracy=0.8671\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5640 - acc: 0.8002 - val_loss: 0.4478 - val_acc: 0.8420\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3996 - acc: 0.8555 - val_loss: 0.4366 - val_acc: 0.8372\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3591 - acc: 0.8694 - val_loss: 0.3554 - val_acc: 0.8690\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3347 - acc: 0.8773 - val_loss: 0.3552 - val_acc: 0.8695\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3185 - acc: 0.8831 - val_loss: 0.3580 - val_acc: 0.8737\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3042 - acc: 0.8876 - val_loss: 0.3311 - val_acc: 0.8790\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2947 - acc: 0.8905 - val_loss: 0.3977 - val_acc: 0.8622\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2843 - acc: 0.8944 - val_loss: 0.3314 - val_acc: 0.8767\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_290 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_291 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_292 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_293 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 44,860\n",
      "Trainable params: 44,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3664 - acc: 0.8689\n",
      "Test dataset: loss=0.3664, accuracy=0.8689\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5755 - acc: 0.7953 - val_loss: 0.4089 - val_acc: 0.8558\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3999 - acc: 0.8543 - val_loss: 0.4146 - val_acc: 0.8493\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3612 - acc: 0.8683 - val_loss: 0.3671 - val_acc: 0.8708\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3374 - acc: 0.8763 - val_loss: 0.3972 - val_acc: 0.8578\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3199 - acc: 0.8822 - val_loss: 0.3639 - val_acc: 0.8673\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3079 - acc: 0.8870 - val_loss: 0.3530 - val_acc: 0.8718\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2967 - acc: 0.8890 - val_loss: 0.3498 - val_acc: 0.8722\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2858 - acc: 0.8938 - val_loss: 0.3411 - val_acc: 0.8830\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2762 - acc: 0.8978 - val_loss: 0.3515 - val_acc: 0.8815\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2665 - acc: 0.8994 - val_loss: 0.3149 - val_acc: 0.8880\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_294 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 47,410\n",
      "Trainable params: 47,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3373 - acc: 0.8782\n",
      "Test dataset: loss=0.3373, accuracy=0.8782\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5844 - acc: 0.7855 - val_loss: 0.4493 - val_acc: 0.8385\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4065 - acc: 0.8520 - val_loss: 0.3950 - val_acc: 0.8557\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3726 - acc: 0.8628 - val_loss: 0.3608 - val_acc: 0.8687\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3455 - acc: 0.8727 - val_loss: 0.3423 - val_acc: 0.8748\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3300 - acc: 0.8777 - val_loss: 0.3589 - val_acc: 0.8680\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3148 - acc: 0.8836 - val_loss: 0.3786 - val_acc: 0.8658\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_299 (Dense)            (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_300 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_301 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_302 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_303 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_304 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 49,960\n",
      "Trainable params: 49,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3762 - acc: 0.8637\n",
      "Test dataset: loss=0.3762, accuracy=0.8637\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5377 - acc: 0.8138 - val_loss: 0.4639 - val_acc: 0.8402\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3990 - acc: 0.8577 - val_loss: 0.3988 - val_acc: 0.8553\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3590 - acc: 0.8711 - val_loss: 0.4063 - val_acc: 0.8512\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3320 - acc: 0.8779 - val_loss: 0.3414 - val_acc: 0.8783\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3115 - acc: 0.8862 - val_loss: 0.3251 - val_acc: 0.8812\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2954 - acc: 0.8916 - val_loss: 0.3217 - val_acc: 0.8840\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2833 - acc: 0.8949 - val_loss: 0.3211 - val_acc: 0.8827\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2755 - acc: 0.8978 - val_loss: 0.3200 - val_acc: 0.8873\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2644 - acc: 0.9027 - val_loss: 0.3237 - val_acc: 0.8812\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2543 - acc: 0.9066 - val_loss: 0.3142 - val_acc: 0.8853\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_305 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3576 - acc: 0.8762\n",
      "Test dataset: loss=0.3576, accuracy=0.8762\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5254 - acc: 0.8162 - val_loss: 0.3770 - val_acc: 0.8660\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3796 - acc: 0.8619 - val_loss: 0.4005 - val_acc: 0.8557\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3430 - acc: 0.8753 - val_loss: 0.3690 - val_acc: 0.8650\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3191 - acc: 0.8827 - val_loss: 0.3202 - val_acc: 0.8833\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3012 - acc: 0.8881 - val_loss: 0.3278 - val_acc: 0.8817\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2856 - acc: 0.8944 - val_loss: 0.3300 - val_acc: 0.8810\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_307 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3552 - acc: 0.8741\n",
      "Test dataset: loss=0.3552, accuracy=0.8741\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5339 - acc: 0.8040 - val_loss: 0.3965 - val_acc: 0.8600\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3773 - acc: 0.8609 - val_loss: 0.3644 - val_acc: 0.8680\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3409 - acc: 0.8746 - val_loss: 0.3437 - val_acc: 0.8740\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3180 - acc: 0.8835 - val_loss: 0.3158 - val_acc: 0.8822\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3028 - acc: 0.8872 - val_loss: 0.3125 - val_acc: 0.8847\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2865 - acc: 0.8932 - val_loss: 0.3239 - val_acc: 0.8838\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2751 - acc: 0.8960 - val_loss: 0.3234 - val_acc: 0.8895\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_310 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_313 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 99,710\n",
      "Trainable params: 99,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3443 - acc: 0.8766\n",
      "Test dataset: loss=0.3443, accuracy=0.8766\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5338 - acc: 0.8065 - val_loss: 0.4146 - val_acc: 0.8482\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3859 - acc: 0.8601 - val_loss: 0.3461 - val_acc: 0.8740\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3472 - acc: 0.8724 - val_loss: 0.3688 - val_acc: 0.8667\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3247 - acc: 0.8799 - val_loss: 0.3270 - val_acc: 0.8810\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3026 - acc: 0.8882 - val_loss: 0.3212 - val_acc: 0.8848\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2894 - acc: 0.8921 - val_loss: 0.3377 - val_acc: 0.8785\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2775 - acc: 0.8966 - val_loss: 0.3087 - val_acc: 0.8893\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2647 - acc: 0.8997 - val_loss: 0.3166 - val_acc: 0.8908\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2549 - acc: 0.9043 - val_loss: 0.3291 - val_acc: 0.8793\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_314 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_315 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_316 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 109,810\n",
      "Trainable params: 109,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3343 - acc: 0.8831\n",
      "Test dataset: loss=0.3343, accuracy=0.8831\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5487 - acc: 0.7986 - val_loss: 0.3974 - val_acc: 0.8615\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3910 - acc: 0.8591 - val_loss: 0.3927 - val_acc: 0.8565\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3525 - acc: 0.8725 - val_loss: 0.3585 - val_acc: 0.8687\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3265 - acc: 0.8800 - val_loss: 0.3267 - val_acc: 0.8855\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3105 - acc: 0.8847 - val_loss: 0.3342 - val_acc: 0.8775\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2955 - acc: 0.8913 - val_loss: 0.3353 - val_acc: 0.8802\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_319 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 119,910\n",
      "Trainable params: 119,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3638 - acc: 0.8744\n",
      "Test dataset: loss=0.3638, accuracy=0.8744\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5219 - acc: 0.8177 - val_loss: 0.4011 - val_acc: 0.8598\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3836 - acc: 0.8622 - val_loss: 0.3534 - val_acc: 0.8720\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3429 - acc: 0.8764 - val_loss: 0.3249 - val_acc: 0.8837\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3146 - acc: 0.8854 - val_loss: 0.3366 - val_acc: 0.8787\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2956 - acc: 0.8907 - val_loss: 0.3118 - val_acc: 0.8882\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2826 - acc: 0.8954 - val_loss: 0.3270 - val_acc: 0.8787\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2685 - acc: 0.8998 - val_loss: 0.3052 - val_acc: 0.8873\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2577 - acc: 0.9034 - val_loss: 0.3174 - val_acc: 0.8848\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2483 - acc: 0.9070 - val_loss: 0.3170 - val_acc: 0.8862\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_325 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3305 - acc: 0.8825\n",
      "Test dataset: loss=0.3305, accuracy=0.8825\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5125 - acc: 0.8170 - val_loss: 0.3975 - val_acc: 0.8595\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3694 - acc: 0.8643 - val_loss: 0.3979 - val_acc: 0.8547\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3338 - acc: 0.8765 - val_loss: 0.3352 - val_acc: 0.8743\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3079 - acc: 0.8858 - val_loss: 0.3378 - val_acc: 0.8765\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2895 - acc: 0.8913 - val_loss: 0.3324 - val_acc: 0.8827\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2761 - acc: 0.8966 - val_loss: 0.3190 - val_acc: 0.8865\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2586 - acc: 0.9025 - val_loss: 0.3269 - val_acc: 0.8882\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2478 - acc: 0.9054 - val_loss: 0.3106 - val_acc: 0.8903\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2363 - acc: 0.9102 - val_loss: 0.3179 - val_acc: 0.8863\n",
      "Epoch 10/10\n",
      "1668/1688 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9129INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2289 - acc: 0.9129 - val_loss: 0.3176 - val_acc: 0.8883\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_327 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3374 - acc: 0.8799\n",
      "Test dataset: loss=0.3374, accuracy=0.8799\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5115 - acc: 0.8122 - val_loss: 0.3826 - val_acc: 0.8652\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3760 - acc: 0.8610 - val_loss: 0.3543 - val_acc: 0.8657\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3333 - acc: 0.8784 - val_loss: 0.3375 - val_acc: 0.8767\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3097 - acc: 0.8849 - val_loss: 0.3264 - val_acc: 0.8832\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2891 - acc: 0.8932 - val_loss: 0.3381 - val_acc: 0.8732\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2752 - acc: 0.8965 - val_loss: 0.3677 - val_acc: 0.8670\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_330 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 239,410\n",
      "Trainable params: 239,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3596 - acc: 0.8748\n",
      "Test dataset: loss=0.3596, accuracy=0.8748\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5161 - acc: 0.8129 - val_loss: 0.4227 - val_acc: 0.8423\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3786 - acc: 0.8616 - val_loss: 0.3670 - val_acc: 0.8687\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3383 - acc: 0.8763 - val_loss: 0.3381 - val_acc: 0.8825\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3159 - acc: 0.8851 - val_loss: 0.3365 - val_acc: 0.8848\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2952 - acc: 0.8907 - val_loss: 0.3443 - val_acc: 0.8775\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2843 - acc: 0.8948 - val_loss: 0.3267 - val_acc: 0.8843\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2708 - acc: 0.8998 - val_loss: 0.3227 - val_acc: 0.8910\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2597 - acc: 0.9039 - val_loss: 0.3123 - val_acc: 0.8915\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2502 - acc: 0.9061 - val_loss: 0.3312 - val_acc: 0.8872\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2405 - acc: 0.9099 - val_loss: 0.2995 - val_acc: 0.8955\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_334 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 279,610\n",
      "Trainable params: 279,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3331 - acc: 0.8849\n",
      "Test dataset: loss=0.3331, accuracy=0.8849\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5467 - acc: 0.7939 - val_loss: 0.4345 - val_acc: 0.8420\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3867 - acc: 0.8597 - val_loss: 0.3521 - val_acc: 0.8713\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3470 - acc: 0.8730 - val_loss: 0.3347 - val_acc: 0.8780\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3247 - acc: 0.8804 - val_loss: 0.3338 - val_acc: 0.8803\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.3288 - val_acc: 0.8868\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2889 - acc: 0.8938 - val_loss: 0.3338 - val_acc: 0.8788\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2821 - acc: 0.8957 - val_loss: 0.3225 - val_acc: 0.8828\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2650 - acc: 0.9023 - val_loss: 0.3091 - val_acc: 0.8940\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2562 - acc: 0.9050 - val_loss: 0.3144 - val_acc: 0.8933\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2457 - acc: 0.9088 - val_loss: 0.3297 - val_acc: 0.8867\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_339 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 319,810\n",
      "Trainable params: 319,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3418 - acc: 0.8793\n",
      "Test dataset: loss=0.3418, accuracy=0.8793\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5152 - acc: 0.8176 - val_loss: 0.3860 - val_acc: 0.8613\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3768 - acc: 0.8645 - val_loss: 0.3591 - val_acc: 0.8687\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3369 - acc: 0.8762 - val_loss: 0.3531 - val_acc: 0.8702\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3127 - acc: 0.8845 - val_loss: 0.3342 - val_acc: 0.8772\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2925 - acc: 0.8925 - val_loss: 0.3145 - val_acc: 0.8863\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2782 - acc: 0.8964 - val_loss: 0.3189 - val_acc: 0.8840\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2630 - acc: 0.9019 - val_loss: 0.3115 - val_acc: 0.8858\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2526 - acc: 0.9047 - val_loss: 0.2958 - val_acc: 0.8932\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2410 - acc: 0.9098 - val_loss: 0.3228 - val_acc: 0.8842\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - ETA: 0s - loss: 0.2318 - acc: 0.9131INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2318 - acc: 0.9131 - val_loss: 0.3071 - val_acc: 0.8902\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_345 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3287 - acc: 0.8827\n",
      "Test dataset: loss=0.3287, accuracy=0.8827\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4975 - acc: 0.8208 - val_loss: 0.4154 - val_acc: 0.8553\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3684 - acc: 0.8659 - val_loss: 0.3751 - val_acc: 0.8642\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3305 - acc: 0.8790 - val_loss: 0.3496 - val_acc: 0.8677\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3057 - acc: 0.8876 - val_loss: 0.3494 - val_acc: 0.8742\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2860 - acc: 0.8946 - val_loss: 0.3136 - val_acc: 0.8847\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2716 - acc: 0.8982 - val_loss: 0.3084 - val_acc: 0.8898\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2584 - acc: 0.9017 - val_loss: 0.3144 - val_acc: 0.8865\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2459 - acc: 0.9069 - val_loss: 0.3015 - val_acc: 0.8893\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2345 - acc: 0.9120 - val_loss: 0.3159 - val_acc: 0.8953\n",
      "Epoch 10/10\n",
      "1671/1688 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9132INFO:tensorflow:Assets written to: class_model.ckpt/assets\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2265 - acc: 0.9134 - val_loss: 0.3216 - val_acc: 0.8960\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_347 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 328,810\n",
      "Trainable params: 328,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3261 - acc: 0.8861\n",
      "Test dataset: loss=0.3261, accuracy=0.8861\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5071 - acc: 0.8156 - val_loss: 0.4172 - val_acc: 0.8427\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3715 - acc: 0.8629 - val_loss: 0.3659 - val_acc: 0.8582\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3346 - acc: 0.8763 - val_loss: 0.3594 - val_acc: 0.8727\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3114 - acc: 0.8839 - val_loss: 0.3493 - val_acc: 0.8752\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2911 - acc: 0.8903 - val_loss: 0.3343 - val_acc: 0.8815\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2796 - acc: 0.8956 - val_loss: 0.3042 - val_acc: 0.8917\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2650 - acc: 0.8999 - val_loss: 0.3324 - val_acc: 0.8815\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2546 - acc: 0.9047 - val_loss: 0.3223 - val_acc: 0.8877\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_350 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 419,110\n",
      "Trainable params: 419,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3314 - acc: 0.8815\n",
      "Test dataset: loss=0.3314, accuracy=0.8815\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5259 - acc: 0.8045 - val_loss: 0.3891 - val_acc: 0.8677\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3820 - acc: 0.8614 - val_loss: 0.4224 - val_acc: 0.8360\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3453 - acc: 0.8736 - val_loss: 0.3486 - val_acc: 0.8760\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3160 - acc: 0.8838 - val_loss: 0.3630 - val_acc: 0.8607\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2994 - acc: 0.8892 - val_loss: 0.3232 - val_acc: 0.8832\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2828 - acc: 0.8948 - val_loss: 0.3438 - val_acc: 0.8777\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2740 - acc: 0.8983 - val_loss: 0.3200 - val_acc: 0.8902\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2602 - acc: 0.9036 - val_loss: 0.3156 - val_acc: 0.8945\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2505 - acc: 0.9067 - val_loss: 0.3122 - val_acc: 0.8873\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2436 - acc: 0.9087 - val_loss: 0.3115 - val_acc: 0.8915\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_354 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_358 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 509,410\n",
      "Trainable params: 509,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3482 - acc: 0.8840\n",
      "Test dataset: loss=0.3482, accuracy=0.8840\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5355 - acc: 0.8028 - val_loss: 0.4068 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3941 - acc: 0.8586 - val_loss: 0.3627 - val_acc: 0.8702\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3517 - acc: 0.8715 - val_loss: 0.3499 - val_acc: 0.8818\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3305 - acc: 0.8775 - val_loss: 0.3565 - val_acc: 0.8748\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3088 - acc: 0.8879 - val_loss: 0.3340 - val_acc: 0.8773\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2945 - acc: 0.8927 - val_loss: 0.3634 - val_acc: 0.8760\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2825 - acc: 0.8965 - val_loss: 0.3195 - val_acc: 0.8860\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2675 - acc: 0.9003 - val_loss: 0.3220 - val_acc: 0.8893\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2596 - acc: 0.9054 - val_loss: 0.3354 - val_acc: 0.8857\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_359 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_360 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_361 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_362 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_363 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_364 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 599,710\n",
      "Trainable params: 599,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3514 - acc: 0.8783\n",
      "Test dataset: loss=0.3514, accuracy=0.8783\n"
     ]
    }
   ],
   "source": [
    "for i in Unit2:\n",
    "    for j in Layer2:\n",
    "        # create a sequential model\n",
    "        model = Sequential()\n",
    "        for k in range(j-1):\n",
    "            model.add(layers.Dense(i, activation=tf.nn.relu, input_dim=input_size))\n",
    "        model.add(layers.Dense(output_size, activation=\"sigmoid\"))\n",
    "        \n",
    "        # compile and fit model\n",
    "        metrics = [ \"acc\" ]\n",
    "        model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=metrics)\n",
    "        model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid),callbacks=callbacks)\n",
    "        \n",
    "        # print summary()\n",
    "        print(model.summary())\n",
    "        \n",
    "        # evaluate model using test set\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "        print(\"Test dataset: loss={tl:5.4f}, accuracy={ta:5.4f}\".format(tl=test_loss, ta=test_accuracy))\n",
    "               \n",
    "        df2.loc[i,j]=test_accuracy\n",
    "        \n",
    "        if accuracy < test_accuracy:\n",
    "            accuracy = test_accuracy\n",
    "            myModel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXlhxAB_bEoc"
   },
   "source": [
    "### Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CsUzXa2rYtN2",
    "outputId": "b388ec86-056a-4b19-8228-512724960bee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>0.8436</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.8394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.8671</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>0.8637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.8741</td>\n",
       "      <td>0.8766</td>\n",
       "      <td>0.8831</td>\n",
       "      <td>0.8744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.8793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.8861</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.8783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2       3       4       5       6\n",
       "10   0.8414  0.8475  0.8436  0.8457  0.8394\n",
       "50   0.8678  0.8671  0.8689  0.8782  0.8637\n",
       "100  0.8762  0.8741  0.8766  0.8831  0.8744\n",
       "200  0.8825  0.8799  0.8748  0.8849  0.8793\n",
       "300  0.8827  0.8861  0.8815   0.884  0.8783"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kI50YQ_2bJW0"
   },
   "source": [
    "### Current Hightest Accuracy and Its Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "iY_CX7LNYwrN",
    "outputId": "5b5d1074-e29b-43df-d699-435f79da51db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860999941825867\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_347 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 328,810\n",
      "Trainable params: 328,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(myModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OK3R4QIIbTYw"
   },
   "source": [
    "### Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "7Im5Z24sYesU",
    "outputId": "6b39e242-2ba4-4a4f-8a3a-2c87deb28e45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEsCAYAAADw/6aoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgmyhBBCCCEqwf8DnowJIn01oRcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in Layer2:\n",
    "    label = '{0} layers'.format(i)\n",
    "    plt.plot(Unit2,df2[i], label=label)\n",
    "plt.xticks(Unit2)\n",
    "plt.title('Keep the number of units same with increasing layer number')\n",
    "plt.legend(fontsize=10)\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5Zs9Ns7bV9l"
   },
   "source": [
    "### Summary: The model overfits when there is a significant drop in test accuracy score, and underfits if there is a significant increase in test accuracy. From the graph we know that, for each different layer-models, they would be underfitted from the number of neurons increase from 10 to 50 , and they underfits from 100 to 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zh5OtRuNVYd5"
   },
   "source": [
    "### Increase the number of units same with increasing layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_K6Q-eRVYd6"
   },
   "outputs": [],
   "source": [
    "Unit3 = [10,50,100,200,300]\n",
    "Layer3 = [3,4,5,6]\n",
    "df3 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tcjxROReVYeF",
    "outputId": "a97d2c84-88ad-45cc-f076-97285b24c708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.6575 - acc: 0.7591 - val_loss: 0.4884 - val_acc: 0.8320\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4526 - acc: 0.8378 - val_loss: 0.4206 - val_acc: 0.8488\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4146 - acc: 0.8501 - val_loss: 0.4184 - val_acc: 0.8503\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3946 - acc: 0.8563 - val_loss: 0.3938 - val_acc: 0.8583\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3808 - acc: 0.8620 - val_loss: 0.3875 - val_acc: 0.8592\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3687 - acc: 0.8658 - val_loss: 0.3842 - val_acc: 0.8630\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3610 - acc: 0.8689 - val_loss: 0.3725 - val_acc: 0.8667\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3533 - acc: 0.8704 - val_loss: 0.3611 - val_acc: 0.8690\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3499 - acc: 0.8732 - val_loss: 0.3717 - val_acc: 0.8652\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3420 - acc: 0.8755 - val_loss: 0.3704 - val_acc: 0.8698\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_365 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_366 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_367 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 8,910\n",
      "Trainable params: 8,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3877 - acc: 0.8608\n",
      "Test dataset: loss=0.3877, accuracy=0.8608\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6308 - acc: 0.7757 - val_loss: 0.4547 - val_acc: 0.8432\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4475 - acc: 0.8398 - val_loss: 0.4018 - val_acc: 0.8578\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4049 - acc: 0.8540 - val_loss: 0.3970 - val_acc: 0.8595\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3841 - acc: 0.8603 - val_loss: 0.3810 - val_acc: 0.8615\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3691 - acc: 0.8652 - val_loss: 0.3774 - val_acc: 0.8663\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3573 - acc: 0.8696 - val_loss: 0.3790 - val_acc: 0.8645\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3490 - acc: 0.8712 - val_loss: 0.3560 - val_acc: 0.8717\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3403 - acc: 0.8749 - val_loss: 0.3652 - val_acc: 0.8703\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3353 - acc: 0.8764 - val_loss: 0.3701 - val_acc: 0.8697\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_368 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_369 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_370 (Dense)            (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_371 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 14,510\n",
      "Trainable params: 14,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3962 - acc: 0.8605\n",
      "Test dataset: loss=0.3962, accuracy=0.8605\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6115 - acc: 0.7764 - val_loss: 0.5000 - val_acc: 0.8113\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4280 - acc: 0.8430 - val_loss: 0.4445 - val_acc: 0.8345\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3926 - acc: 0.8545 - val_loss: 0.3769 - val_acc: 0.8622\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3718 - acc: 0.8624 - val_loss: 0.3598 - val_acc: 0.8647\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3586 - acc: 0.8668 - val_loss: 0.3608 - val_acc: 0.8647\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3479 - acc: 0.8707 - val_loss: 0.3869 - val_acc: 0.8595\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_372 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_373 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_374 (Dense)            (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_375 (Dense)            (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 35,710\n",
      "Trainable params: 35,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4011 - acc: 0.8520\n",
      "Test dataset: loss=0.4011, accuracy=0.8520\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6024 - acc: 0.7750 - val_loss: 0.4505 - val_acc: 0.8315\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4300 - acc: 0.8435 - val_loss: 0.3907 - val_acc: 0.8552\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3975 - acc: 0.8539 - val_loss: 0.3847 - val_acc: 0.8615\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3833 - acc: 0.8597 - val_loss: 0.3961 - val_acc: 0.8555\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3676 - acc: 0.8655 - val_loss: 0.3835 - val_acc: 0.8575\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3572 - acc: 0.8684 - val_loss: 0.4387 - val_acc: 0.8492\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3498 - acc: 0.8710 - val_loss: 0.3794 - val_acc: 0.8603\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3408 - acc: 0.8743 - val_loss: 0.3676 - val_acc: 0.8657\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3362 - acc: 0.8754 - val_loss: 0.3839 - val_acc: 0.8615\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3321 - acc: 0.8765 - val_loss: 0.3859 - val_acc: 0.8645\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_377 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_380 (Dense)            (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 97,010\n",
      "Trainable params: 97,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3996 - acc: 0.8544\n",
      "Test dataset: loss=0.3996, accuracy=0.8544\n"
     ]
    }
   ],
   "source": [
    "for j in Layer3:\n",
    "    # create a sequential model\n",
    "    model = Sequential()\n",
    "    i = 0\n",
    "    for k in range(j-1):\n",
    "        hidden = Unit3[i]\n",
    "        model.add(layers.Dense(hidden, activation=tf.nn.relu, input_dim=input_size))\n",
    "        i += 1\n",
    "    model.add(layers.Dense(output_size, activation=\"sigmoid\"))\n",
    "\n",
    "    # compile and fit model\n",
    "    metrics = [ \"acc\" ]\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=metrics)\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid),callbacks=callbacks)\n",
    "\n",
    "    # print summary()\n",
    "    print(model.summary())\n",
    "\n",
    "    # evaluate model using test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Test dataset: loss={tl:5.4f}, accuracy={ta:5.4f}\".format(tl=test_loss, ta=test_accuracy))\n",
    "\n",
    "    df3[j]=test_accuracy\n",
    "        \n",
    "    if accuracy < test_accuracy:\n",
    "        accuracy = test_accuracy\n",
    "        myModel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRPMQi-cdAkv"
   },
   "source": [
    "### Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5BkwKQwDdHze",
    "outputId": "163bbdab-922b-498f-be77-1d66966f5603"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.86080002784729,\n",
       " 4: 0.8604999780654907,\n",
       " 5: 0.8519999980926514,\n",
       " 6: 0.8543999791145325}"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gr8ci0lGmWOG",
    "outputId": "2edd24cc-074f-4e98-be10-4ebfba5a08a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>0.8436</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.8394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.8671</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>0.8637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.8741</td>\n",
       "      <td>0.8766</td>\n",
       "      <td>0.8831</td>\n",
       "      <td>0.8744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.8793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.8861</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.8783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2       3       4       5       6\n",
       "10   0.8414  0.8475  0.8436  0.8457  0.8394\n",
       "50   0.8678  0.8671  0.8689  0.8782  0.8637\n",
       "100  0.8762  0.8741  0.8766  0.8831  0.8744\n",
       "200  0.8825  0.8799  0.8748  0.8849  0.8793\n",
       "300  0.8827  0.8861  0.8815   0.884  0.8783"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with df2\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "job2Hy5edHny"
   },
   "source": [
    "### Current Highest Accuracy and Its Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "2UQVEIGadHc-",
    "outputId": "c41f96d0-b8a3-4aa7-80ad-bfd0349c353b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860999941825867\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_347 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 328,810\n",
      "Trainable params: 328,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(myModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ML2aNzZIdRS6"
   },
   "source": [
    "### Summary: in this model, the number of unites in each layer of each model is following: 2L-Model=[10,10], 3L-Model=[10,50,10], 4L-Model=[10,50,100,10], 5L-Model=[10,50,100,200,10], 6L-Model=[10,50,100,200,300,10].\n",
    "\n",
    "### As we see, when neurons increase, 3L-Model underfits with same neuron number in each layer and has better performance in increasing neuron one; and with the same analysis, 4L-Model and 5L-Model overfits with increasing neurons models, but overall to 6L-Model, increasing neuron model overfits as well even though it underfits with 10 neurons in each layer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5SWuj30VYeQ"
   },
   "source": [
    "### Decrease the number of units same with increasing layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xa__k7ITVYeR"
   },
   "outputs": [],
   "source": [
    "Unit4 = [300,200,100,50,10]\n",
    "Layer4 = [3,4,5,6]\n",
    "df4 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SNxTtmj5qCZu",
    "outputId": "613e46af-ef98-4a28-dd8f-43165514c83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5053 - acc: 0.8178 - val_loss: 0.3896 - val_acc: 0.8590\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3716 - acc: 0.8630 - val_loss: 0.3540 - val_acc: 0.8730\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3316 - acc: 0.8772 - val_loss: 0.3474 - val_acc: 0.8768\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3095 - acc: 0.8857 - val_loss: 0.3347 - val_acc: 0.8792\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2875 - acc: 0.8924 - val_loss: 0.3417 - val_acc: 0.8787\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2717 - acc: 0.8991 - val_loss: 0.2982 - val_acc: 0.8937\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2585 - acc: 0.9023 - val_loss: 0.3275 - val_acc: 0.8795\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2473 - acc: 0.9072 - val_loss: 0.3044 - val_acc: 0.8925\n",
      "Model: \"sequential_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_383 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_385 (Dense)            (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 297,710\n",
      "Trainable params: 297,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3374 - acc: 0.8800\n",
      "Test dataset: loss=0.3374, accuracy=0.8800\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5040 - acc: 0.8199 - val_loss: 0.3850 - val_acc: 0.8602\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3736 - acc: 0.8620 - val_loss: 0.3980 - val_acc: 0.8547\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3370 - acc: 0.8761 - val_loss: 0.3444 - val_acc: 0.8753\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3106 - acc: 0.8845 - val_loss: 0.3223 - val_acc: 0.8810\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2944 - acc: 0.8909 - val_loss: 0.3169 - val_acc: 0.8863\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2737 - acc: 0.8985 - val_loss: 0.3225 - val_acc: 0.8842\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2611 - acc: 0.9012 - val_loss: 0.3093 - val_acc: 0.8887\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2486 - acc: 0.9050 - val_loss: 0.3288 - val_acc: 0.8817\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2401 - acc: 0.9089 - val_loss: 0.3101 - val_acc: 0.8892\n",
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_386 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 316,810\n",
      "Trainable params: 316,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3347 - acc: 0.8834\n",
      "Test dataset: loss=0.3347, accuracy=0.8834\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5184 - acc: 0.8118 - val_loss: 0.4192 - val_acc: 0.8407\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3762 - acc: 0.8618 - val_loss: 0.3628 - val_acc: 0.8615\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3398 - acc: 0.8754 - val_loss: 0.3297 - val_acc: 0.8803\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3147 - acc: 0.8823 - val_loss: 0.3152 - val_acc: 0.8847\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2965 - acc: 0.8898 - val_loss: 0.3294 - val_acc: 0.8760\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2806 - acc: 0.8967 - val_loss: 0.3156 - val_acc: 0.8900\n",
      "Model: \"sequential_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_390 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 321,360\n",
      "Trainable params: 321,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3504 - acc: 0.8750\n",
      "Test dataset: loss=0.3504, accuracy=0.8750\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5614 - acc: 0.7888 - val_loss: 0.4366 - val_acc: 0.8420\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3784 - acc: 0.8616 - val_loss: 0.3791 - val_acc: 0.8602\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3437 - acc: 0.8749 - val_loss: 0.3525 - val_acc: 0.8712\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3174 - acc: 0.8835 - val_loss: 0.3257 - val_acc: 0.8827\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2974 - acc: 0.8909 - val_loss: 0.3306 - val_acc: 0.8797\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2841 - acc: 0.8945 - val_loss: 0.3059 - val_acc: 0.8895\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.2686 - acc: 0.8981 - val_loss: 0.3319 - val_acc: 0.8813\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2581 - acc: 0.9027 - val_loss: 0.3174 - val_acc: 0.8843\n",
      "Model: \"sequential_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_395 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_396 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_397 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_398 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_399 (Dense)            (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_400 (Dense)            (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 321,470\n",
      "Trainable params: 321,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3454 - acc: 0.8779\n",
      "Test dataset: loss=0.3454, accuracy=0.8779\n"
     ]
    }
   ],
   "source": [
    "for j in Layer4:\n",
    "    # create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    for k in range(j-1):\n",
    "        hidden = Unit4[k]\n",
    "        model.add(layers.Dense(hidden, activation=tf.nn.relu, input_dim=input_size))\n",
    "        \n",
    "    model.add(layers.Dense(output_size, activation=\"sigmoid\"))\n",
    "\n",
    "    # compile and fit model\n",
    "    metrics = [ \"acc\" ]\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=metrics)\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid),callbacks=callbacks)\n",
    "\n",
    "    # print summary()\n",
    "    print(model.summary())\n",
    "\n",
    "    # evaluate model using test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Test dataset: loss={tl:5.4f}, accuracy={ta:5.4f}\".format(tl=test_loss, ta=test_accuracy))\n",
    "\n",
    "    df4[j]=test_accuracy\n",
    "        \n",
    "    if accuracy < test_accuracy:\n",
    "        accuracy = test_accuracy\n",
    "        myModel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37xCs3sXofYk"
   },
   "source": [
    "### Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AjHmQkZ-ofJv",
    "outputId": "4b757444-ac66-4c88-baf0-1a6aa60ba22a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.8799999952316284, 4: 0.883400022983551, 5: 0.875, 6: 0.8779000043869019}"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sHZ2f9ruoe_l",
    "outputId": "b2a5f3e2-d143-4ac5-bfe1-6121d25bfd56"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>0.8436</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.8394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.8671</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>0.8637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.8741</td>\n",
       "      <td>0.8766</td>\n",
       "      <td>0.8831</td>\n",
       "      <td>0.8744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.8793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.8861</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.8783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2       3       4       5       6\n",
       "10   0.8414  0.8475  0.8436  0.8457  0.8394\n",
       "50   0.8678  0.8671  0.8689  0.8782  0.8637\n",
       "100  0.8762  0.8741  0.8766  0.8831  0.8744\n",
       "200  0.8825  0.8799  0.8748  0.8849  0.8793\n",
       "300  0.8827  0.8861  0.8815   0.884  0.8783"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with df2\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H3FU-KFoe2L"
   },
   "source": [
    "### Current Highest Accuracy and Its Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "K8O1nOSroebX",
    "outputId": "a86ec460-3c88-4d59-9006-2cfba6ff69a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860999941825867\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_347 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 328,810\n",
      "Trainable params: 328,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(myModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWYXDYNqozyA"
   },
   "source": [
    "### Summary: in this model, the number of unites in each layer of each model is following: 3L-Model=[300,200,10], 4L-Model=[300,200,100,10], 5L-Model=[300,200,100,50,10], 6L-Model=[300,200,100,50,10,10].\n",
    "\n",
    "### As we see, when neurons increase, 3L-Model overfits with same neuron number (300) in each layer and has better performance in decreasing neuron one; and with the same analysis, 4L-Model, 5L-Model, 6L-Model underfits with decreasing neurons models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULtRZT4EVYeu"
   },
   "source": [
    "**The Information of the Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "mkLDLsMzVYev",
    "outputId": "2b4f2280-716d-4bfd-90d0-17f07ac1c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860999941825867\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_347 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 328,810\n",
      "Trainable params: 328,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(myModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gj4KdnT8VYe6"
   },
   "source": [
    "## Additional Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rX3AJuYsVYe9",
    "outputId": "75bae0bc-16c1-4743-e2f9-5902a526723f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./assignment3_best_model already exists, files will be over-written.\n",
      "Model saved in directory ./assignment3_best_model; create an archive of this directory and submit with your assignment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "modelName = \"assignment3_best_model\"\n",
    "model_path = os.path.join(\".\", modelName)     \n",
    "\n",
    "def saveModel(model, model_path): \n",
    "    try:\n",
    "        os.makedirs(model_path)\n",
    "    except OSError:\n",
    "        print(\"Directory {dir:s} already exists, files will be over-written.\".format(dir=model_path))\n",
    "        \n",
    "    # Save JSON config to disk\n",
    "    json_config = model.to_json()\n",
    "    with open(os.path.join(model_path, 'config.json'), 'w') as json_file:\n",
    "        json_file.write(json_config)\n",
    "    # Save weights to disk\n",
    "    model.save_weights(os.path.join(model_path, 'weights.h5'))\n",
    "    \n",
    "    print(\"Model saved in directory {dir:s}; create an archive of this directory and submit with your assignment.\".format(dir=model_path))\n",
    "    \n",
    "def loadModel(model_path):\n",
    "    # Reload the model from the 2 files we saved\n",
    "    with open(os.path.join(model_path, 'config.json')) as json_file:\n",
    "        json_config = json_file.read()\n",
    "    model = tf.keras.models.model_from_json(json_config)\n",
    "    model.load_weights(os.path.join(model_path, 'weights.h5'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def MyModel(X_test, Y_test, model_path):\n",
    "    # YOU MAY NOT change model after this statement !\n",
    "    model = loadModel(model_path)\n",
    "    \n",
    "    # It should run model to create an array of predictions; we initialize it to the empty array for convenience\n",
    "    predictions = []\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    predictions = model.predict(X_test).argmax(1)\n",
    "    \n",
    "    confusion = confusion_matrix(predictions,y_test)\n",
    "    df_cm = pd.DataFrame(confusion,index = [i for i in range(1, 11)],\n",
    "                  columns = [i for i in range(1,11)])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm,annot = True, cmap=\"Blues\")\n",
    "    plt.show()\n",
    "    print(model.summary())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "# Determine \n",
    "# - the dimensions of the input by examining the first training example\n",
    "# - the dimensions of the output (number of classes) by examinimg the targets\n",
    "input_size = np.prod(X_train[0].shape)\n",
    "output_size = np.unique(y_train).shape[0]\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = X_train[0].shape[0:2]\n",
    "\n",
    "valid_size = X_train.shape[0] // 10\n",
    "\n",
    "# Flatten the data to one dimension and normalize to range [0,1]\n",
    "X_train = X_train.astype(np.float32).reshape(-1, input_size) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, input_size) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:valid_size], X_train[valid_size:]\n",
    "y_valid, y_train = y_train[:valid_size], y_train[valid_size:]\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "#saveModel(myModel, model_path)\n",
    "\n",
    "# prediction\n",
    "MyModel(X_test,y_test,model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xn8MDRfpVYfI",
    "outputId": "df294226-3ad8-4c49-ac9a-8ba4931c17c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yk81rearVYfR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_jh6011.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
